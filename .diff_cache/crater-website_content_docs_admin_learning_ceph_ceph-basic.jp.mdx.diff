diff --git crater-website/content/docs/admin/learning/ceph/ceph-basic.jp.mdx crater-website/content/docs/admin/learning/ceph/ceph-basic.jp.mdx
index 87e676c..e8c7ec5 100644
--- crater-website/content/docs/admin/learning/ceph/ceph-basic.jp.mdx
+++ crater-website/content/docs/admin/learning/ceph/ceph-basic.jp.mdx
@@ -7,13 +7,13 @@ description: "Rook-Ceph 入門ガイド、初心者向けに作成しました
 
 ## 一、前提知識
 
-Rook-Ceph の実践に進む前に、2つの重要な概念、**Ceph** と **Rook-Ceph** の根本的な理解が必要です。これらはそれぞれ、分散ストレージシステムの核心的な能力と、Kubernetes 上での自動運用ソリューションを表しています。
+Rook-Ceph の実践に進む前に、2 つの重要な概念、**Ceph** と **Rook-Ceph** の根本的な理解が必要です。これらはそれぞれ、分散ストレージシステムの核心的な能力と、Kubernetes 上での自動運用ソリューションを表しています。
 
 ### 1. Ceph
 
 **Ceph はオープンソースの分散ストレージシステム**であり、Sage Weil が開始し、大規模なストレージ環境におけるデータの一貫性、可用性、拡張性の問題を解決することを目的としています。従来の集中型ストレージソリューションとは異なり、Ceph は単一のコントローラーに依存せず、ペアノード間の協調に基づいて構築されています。Ceph の設計目標には以下があります。
 
-* **高拡張性**：数TBから数PBまでのストレージ容量の増加をサポート；
+* **高拡張性**：数 TB から数 PB までのストレージ容量の増加をサポート；
 * **高可用性と耐障害性**：レプリカまたはエラー訂正コードのメカニズムにより、ノードがダウンしてもデータが利用可能に保証；
 * **統一ストレージモデル**：ブロックデバイス（Block）、オブジェクトストレージ（Object）、ファイルシステム（File）のすべてのアクセス方式をサポート。
 
@@ -26,7 +26,7 @@ Ceph クラスターには主に以下の核心コンポーネントが含まれ
 | MGR（Manager）               | モニタリング、統計、拡張機能を提供             |
 | MDS（Metadata Server、オプション） | ファイルシステムをサポートするためにディレクトリツリーのメタデータを管理 |
 
-Ceph クラスターは通常、3つ以上の MON が必要で、選出の安全性を確保し、複数の OSD が各ノードの物理ディスクにマウントされ、データストレージ能力を共同で提供します。
+Ceph クラスターは通常、3 つ以上の MON が必要で、選出の安全性を確保し、複数の OSD が各ノードの物理ディスクにマウントされ、データストレージ能力を共同で提供します。
 
 ### 2. Rook-Ceph
 
@@ -56,7 +56,7 @@ Rook-Ceph は真のクラウドナティブストレージソリューション
 
 ## 二、学習の流れ
 
-Ceph と Rook-Ceph には非常に複雑な内容が含まれており、初心者にとっては初期段階における最大の困難は資料が不足しているのではなく、資料が多く、体系的かつ段階的なものではないということです。効率を高め、無駄な失敗を避けるために、学習の流れを3つの段階に分けて、実践を通じて関連するツールに関する理解を築くことをお勧めします。
+Ceph と Rook-Ceph には非常に複雑な内容が含まれており、初心者にとっては初期段階における最大の困難は資料が不足しているのではなく、資料が多く、体系的かつ段階的なものではないということです。効率を高め、無駄な失敗を避けるために、学習の流れを 3 つの段階に分けて、実践を通じて関連するツールに関する理解を築くことをお勧めします。
 
 ### 1. 階段一：概念の構築
 
@@ -73,7 +73,7 @@ Ceph と Rook-Ceph には非常に複雑な内容が含まれており、初心
 | Ceph はどのような問題を解決するために生まれたのですか？      | Ceph と従来の集中型ストレージシステムの根本的な違いを簡潔に説明してください。 | Ceph が分散 + 容錯 + 非中央集権的なコア思想を明確に理解するため。       |
 | Ceph と Rook-Ceph はどのような関係ですか？                   | Kubernetes で Rook-Ceph を使用すると Ceph を使っていることになりますか？Rook は何をしていますか？ | Rook が Ceph の運用ロジックをカプセル化しているという概念を理解し、両者を混同しないようにするため。 |
 | Ceph の核心コンポーネントは何か？それぞれの責任は何ですか？ | MON、OSD、MGR、MDS の役割と相互作用を説明してください。     | 後のすべてのコマンド操作と障害診断の入口を明確にするため。           |
-| RBD、オブジェクトストレージ、ファイルシステムとは何ですか？ Kubernetes ではどれを使用していますか？ | Kubernetes で PVC を使用する際、Ceph のブロックストレージなのか、それ以外の形式なのか？ | Ceph がブロックストレージなのか、クラウドディスクなど他の概念を含むのかを明確にするため。 |
+| RBD、オブジェクトストレージ、ファイルシステムとは何ですか？Kubernetes ではどれを使用していますか？ | Kubernetes で PVC を使用する際、Ceph のブロックストレージなのか、それ以外の形式なのか？ | Ceph がブロックストレージなのか、クラウドディスクなど他の概念を含むのかを明確にするため。 |
 | なぜ Kubernetes で Ceph をデプロイするには Rook を使用する必要がありますか？ | Rook 以外で Ceph を直接デプロイすることは可能ですか？その違いは何ですか？ | Kubernetes と伝統的アーキテクチャの違い、特に宣言的リソースという思想を理解するため。 |
 
 ### 2. 階段二：環境の習熟
@@ -151,13 +151,13 @@ Ceph と Rook-Ceph には非常に複雑な内容が含まれており、初心
 | pool の詳細を確認 | `ceph osd pool get <pool> all`                               | レプリカ数、符号方式などのパラメータを確認する |
 | pool を作成      | `ceph osd pool create mypool 128 128`                        | 実験環境で異なるサイズの pool を試す |
 | pool を削除      | `ceph osd pool delete mypool mypool --yes-i-really-really-mean-it` | 小心に使用し、実験環境専用に限定する |
-| レプリカ数を3に設定 | `ceph osd pool set mypool size 3`                            | データの冗長性を保証するが、スペースの消費が増加する |
+| レプリカ数を 3 に設定 | `ceph osd pool set mypool size 3`                            | データの冗長性を保証するが、スペースの消費が増加する |
 
 ### 4. RBD 操作練習
 
 | 操作          | コマンド                                         | 用途説明                            |
 | ------------- | -------------------------------------------- | ----------------------------------- |
-| RBDイメージを作成 | `rbd create myrbd --size 1024 --pool mypool` | MB単位で、PVCにバインドする前にイメージを作成する |
+| RBD イメージを作成 | `rbd create myrbd --size 1024 --pool mypool` | MB 単位で、PVC にバインドする前にイメージを作成する |
 | イメージを確認      | `rbd ls -p mypool`                           | 成功的に作成されたかを確認する         |
 | イメージの情報を取得 | `rbd info mypool/myrbd`                      | イメージのサイズや使用状況を確認する     |
 | イメージを削除      | `rbd rm mypool/myrbd`                        | 実験用イメージを削除し、スペースを解放する |
@@ -168,7 +168,7 @@ Ceph と Rook-Ceph には非常に複雑な内容が含まれており、初心
 
 | 操作             | コマンド                                       | 用途説明                        |
 | ---------------- | ------------------------------------------ | ------------------------------- |
-| ファイルシステムを作成 | `ceph fs new myfs myfs_metadata myfs_data` | NFSのような共有マウントをサポート |
+| ファイルシステムを作成 | `ceph fs new myfs myfs_metadata myfs_data` | NFS のような共有マウントをサポート |
 | 全ファイルシステムを確認 | `ceph fs ls`                               | ファイルシステムが有効かどうかを確認する |
 | ファイルシステムの状態を確認 | `ceph fs status`                           | アクティブな MDS ノードとクライアント接続数を確認する |
 
@@ -187,7 +187,7 @@ Ceph と Rook-Ceph には非常に複雑な内容が含まれており、初心
 
 Ceph の生産環境での耐障害能力をシミュレーションするため、OSD ノードの異常オフラインをシミュレーションし、Ceph の自己修復メカニズムが効果的に機能するかを観察する必要があります。
 
-1. 現存のクラスターから1つの OSD を選び、それを `out` にマークし、ディスクがオフラインになることをシミュレーションします；
+1. 現存のクラスターから 1 つの OSD を選び、それを `out` にマークし、ディスクがオフラインになることをシミュレーションします；
 2. Ceph がデータの移動、再バランス、健康状態の変化をトリガーするかを観察します；
 3. OSD tree とクラスターの状態変化を確認し、レプリカの再構築と容量の再分布プロセスを記録します；
 4. 故障を修復した後、OSD を `in` にマークし、システムの回復プロセスを観察します。
@@ -216,7 +216,7 @@ Ceph の生産環境での耐障害能力をシミュレーションするため
 
 1. toolbox を使って各 Pool の使用率とレプリカ構成を確認します；
 2. 現在の Pool に空き状態で未バインドされている RBD イメージがあるかを分析し、リサイクルを検討します；
-3. 実際には容量が不足している場合、いくつかの Pool のレプリカ数を3から2に減少させます（これはテストシナリオに限ります）；
+3. 実際には容量が不足している場合、いくつかの Pool のレプリカ数を 3 から 2 に減少させます（これはテストシナリオに限ります）；
 4. エクスパンションの可能性を評価し、新しいノードや OSD を追加し、システムが自動的に再バランスするかを観察します。
 
 ### 6. シナリオ六：使用されていないイメージと Pool をクリーンアップし、ストレージスペースを解放する
