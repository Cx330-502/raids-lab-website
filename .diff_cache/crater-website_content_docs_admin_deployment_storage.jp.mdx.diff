diff --git crater-website/content/docs/admin/deployment/storage.jp.mdx crater-website/content/docs/admin/deployment/storage.jp.mdx
index 81b3c90..146e65a 100644
--- crater-website/content/docs/admin/deployment/storage.jp.mdx
+++ crater-website/content/docs/admin/deployment/storage.jp.mdx
@@ -1,49 +1,49 @@
 ---
 title: ストレージアーキテクチャ
-description: Craterは、ハイパフォーマンスなローカルワークロードと、Podおよびノード間での持続的な共有データアクセスを処理するため、ハイブリッドストレージアーキテクチャを使用しています。このドキュメントでは、クラスターで使用されているストレージソリューションについて説明します。
+description: Crater は、ハイパフォーマンスなローカルワークロードと、Pod およびノード間での持続的な共有データアクセスを処理するため、ハイブリッドストレージアーキテクチャを使用しています。このドキュメントでは、クラスターで使用されているストレージソリューションについて説明します。
 ---
 
-## 1. ローカル永続ボリューム (OpenEBS経由のLocalPV)
+## 1. ローカル永続ボリューム (OpenEBS 経由の LocalPV)
 
 [OpenEBS LocalPV](https://openebs.io/docs/user-guides/localpv)を使用して、高スループットとデータローカリティを必要とするワークロード用のノードローカルストレージを管理します。
 
-### なぜLocalPVなのか？
+### なぜ LocalPV なのか？
 
-- **CRDベースの管理**: Kubernetesネイティブな宣言的管理でローカルディスクを管理可能。
+- **CRD ベースの管理**: Kubernetes ネイティブな宣言的管理でローカルディスクを管理可能。
 - **性能**: データは同じノード上に残り、ネットワークオーバーヘッドを最小限に抑える。
-- **Craterでの用途**:
+- **Crater での用途**:
   - ジョブキャッシュディレクトリ
   - ローカルデータセットステージング領域
   - ノードごとの推論またはトレーニング用の一時領域
 
 ### StorageClass
 
-ローカルディスクのプロビジョニングに専用の`StorageClass`が設定されています。このクラスは以下のような他のチャートから参照されます:
+ローカルディスクのプロビジョニングに専用の`StorageClass`が設定されています。このクラスは以下のような他のチャートから参照されます：
 - データベースストレージ用の`cloudnative-pg`
 - レプリケーションを必要としない分散ジョブ出力
 
-参照: [`deployments/openebs`](../deployments/openebs)
+参照：[`deployments/openebs`](../deployments/openebs)
 
 ---
 
-## 2. 共有ブロックストレージ (Rook経由のCeph RBD)
+## 2. 共有ブロックストレージ (Rook 経由の Ceph RBD)
 
-複数ノードでアクセス可能な永続ボリュームが必要な場合、Craterは[Rook-Ceph RBD](https://rook.io/docs/rook/latest/ceph-block.html)を使用します。RBDボリュームは動的にプロビジョニングされ、以下の機能をサポートしています:
+複数ノードでアクセス可能な永続ボリュームが必要な場合、Crater は[Rook-Ceph RBD](https://rook.io/docs/rook/latest/ceph-block.html)を使用します。RBD ボリュームは動的にプロビジョニングされ、以下の機能をサポートしています：
 
-- マイグレーションをサポートするReadWriteOnceアクセス
+- マイグレーションをサポートする ReadWriteOnce アクセス
 - レプリケーションと障害復旧
-- 以下に適しています:
-  - Prometheus TSDBストレージ
-  - 永続性を必要とするCraterの内部サービス
+- 以下に適しています：
+  - Prometheus TSDB ストレージ
+  - 永続性を必要とする Crater の内部サービス
   - ノード間で共有されるデータセット
 
-Cephが選ばれた理由は以下の通りです:
+Ceph が選ばれた理由は以下の通りです：
 
-- Rook経由でKubernetesネイティブなプロビジョニングが可能
+- Rook 経由で Kubernetes ネイティブなプロビジョニングが可能
 - 強力なコミュニティサポート
 - スケーラビリティと障害耐性
 
-> 📌 Prometheusなどの多くのステートフルコンポーネントは、カスタムの`StorageClass`を介してCeph RBDボリュームを使用しています。
+> 📌 Prometheus などの多くのステートフルコンポーネントは、カスタムの`StorageClass`を介して Ceph RBD ボリュームを使用しています。
 
 ---
 
@@ -54,15 +54,15 @@ Cephが選ばれた理由は以下の通りです:
 | PostgreSQL (Crater DB) | 永続的               | LocalPV (OpenEBS)            | 高速、ノード特有                     |
 | Prometheus TSDB        | 永続的               | Ceph RBD (Rook)              | マルチノード、高耐久性               |
 | ユーザージョブ         | 一時的 / 永続的      | LocalPV / Ceph RBD           | 設定に基づく                         |
-| Grafanaダッシュボード   | 一時的 / 永続的      | Ceph RBD                     | オプション、ダッシュボード設定に依存 |
+| Grafana ダッシュボード   | 一時的 / 永続的      | Ceph RBD                     | オプション、ダッシュボード設定に依存 |
 
 ---
 
 ## メモ
 
-- LocalPVを提供するノードには、常にラベル付けおよびマウントされたディスクパスを確保してください。
-- Ceph RBDには、クラスターノード上の事前にプロビジョニングされたブロックストレージデバイスが必要です。
-- LocalPVを使用する際は、ノードアフィニティとターレレーションを使用してPodを正しいストレージ場所にバインドしてください。
+- LocalPV を提供するノードには、常にラベル付けおよびマウントされたディスクパスを確保してください。
+- Ceph RBD には、クラスターノード上の事前にプロビジョニングされたブロックストレージデバイスが必要です。
+- LocalPV を使用する際は、ノードアフィニティとターレレーションを使用して Pod を正しいストレージ場所にバインドしてください。
 - 各チャートのドキュメントを参照して、適切な`StorageClass`の上書き方法をご確認ください。
 
 ---
@@ -75,6 +75,6 @@ Cephが選ばれた理由は以下の通りです:
 
 ## インストール
 
-NFSをストレージプロビジョーナーとして使用し、Craterで事前に設定された値を使用した公式Helmチャートを使用することを推奨します。
+NFS をストレージプロビジョーナーとして使用し、Crater で事前に設定された値を使用した公式 Helm チャートを使用することを推奨します。
  
-📖 詳細ガイド: [`deployments/nfs/README.md`](../deployments/nfs/README.md)
\ No newline at end of file
+📖 詳細ガイド：[`deployments/nfs/README.md`](../deployments/nfs/README.md)
\ No newline at end of file
